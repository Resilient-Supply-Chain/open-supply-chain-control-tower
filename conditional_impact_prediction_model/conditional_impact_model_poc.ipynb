{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc4374f",
   "metadata": {},
   "source": [
    "# Conditional Power Outage Impact Model — Proof of Concept\n",
    "\n",
    "This notebook presents a **rapid proof-of-concept (PoC)** for an **outage impact model** developed as a supporting component within a larger end-to-end outage risk and impact modeling system.\n",
    "\n",
    "The objective of this PoC is **not to deliver a production-ready model**, but to quickly validate:\n",
    "- whether a **weather-driven impact signal** exists once an outage has occurred, and  \n",
    "- whether such a component can be reasonably integrated into a broader modeling framework.\n",
    "\n",
    "Given this is exploratory phase of work, the implementation intentionally prioritizes **clarity, robustness, and reproducibility** over sophistication or exhaustive optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Scope\n",
    "\n",
    "The model focuses on **conditional impact estimation**:\n",
    "\n",
    "> *Given that an outage has occurred on a county-day, estimate the number of affected customers.*\n",
    "\n",
    "Key characteristics:\n",
    "- Granularity: **county-day**\n",
    "- Samples: limited (~200+ positive outage days)\n",
    "- Target: total customers affected per county-day\n",
    "- Inputs: aggregated weather and contextual features\n",
    "\n",
    "This PoC **does not model outage occurrence probability** and should not be interpreted as estimating unconditional expected impact. For that purpose, please combine with the other occurrence probability model.\n",
    "\n",
    "---\n",
    "\n",
    "## Modeling Approach\n",
    "\n",
    "Two modeling approaches are evaluated sequentially:\n",
    "\n",
    "1. **Regularized linear regression (Ridge)**  \n",
    "   - Serves as a transparent baseline  \n",
    "   - Highlights the limitations of linear assumptions under small sample size and long-tailed impact distributions\n",
    "\n",
    "2. **Gradient-boosted decision trees (XGBoost)**  \n",
    "   - Introduced to capture non-linear weather–impact relationships  \n",
    "   - Evaluated using time-based cross-validation  \n",
    "   - Selected for final prediction due to consistently lower error metrics\n",
    "\n",
    "To stabilize training:\n",
    "- The target is modeled in **log(1 + customers)** space\n",
    "- Simple imputation and one-hot encoding are applied\n",
    "- Conservative model complexity is enforced to reduce overfitting risk\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations of This PoC\n",
    "\n",
    "This implementation makes several simplifying assumptions and has clear limitations:\n",
    "\n",
    "- **Small sample size** limits statistical power and generalization\n",
    "- **County-day aggregation** obscures event-level heterogeneity\n",
    "- Weather features are coarse and may not align precisely with outage timing\n",
    "- Model evaluation focuses on error reduction rather than probabilistic calibration or tail risk guarantees\n",
    "\n",
    "These limitations are expected and acceptable at this stage, given the PoC’s role within a larger system.\n",
    "\n",
    "---\n",
    "\n",
    "## Planned Improvements\n",
    "\n",
    "Future iterations would focus on:\n",
    "\n",
    "- Integrating with **outage risk model** to estimate unconditional expected impact\n",
    "- Moving from county-day to **event-level or feeder-level granularity**\n",
    "- Refining weather feature windows around outage start times\n",
    "- Improving tail modeling (e.g., high-impact day classification or quantile regression)\n",
    "- Expanding training data across additional seasons and storm regimes\n",
    "\n",
    "---\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "The results of this PoC notebook are intended for:\n",
    "- Architecture validation within a broader outage risk framework\n",
    "- Early signal assessment and model feasibility testing\n",
    "- Guiding prioritization of future data and modeling investments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab7b3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac133f",
   "metadata": {},
   "source": [
    "## 1. Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478b0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Read data\n",
    "# -----------------------------\n",
    "PATH = \"cleaned_primary_table/conditional_impact_primary_table.csv\"  # change if needed\n",
    "df = pd.read_csv(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4da651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) Basic cleanup / typing\n",
    "# -----------------------------\n",
    "# Ensure date is datetime and sort (critical for time-based CV)\n",
    "if \"date\" in df.columns:\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "else:\n",
    "    raise ValueError(\"Expected a 'date' column in the dataset.\")\n",
    "\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Target\n",
    "TARGET = \"total_customers\"\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"Expected target column '{TARGET}' in the dataset.\")\n",
    "\n",
    "# Optional: remove impossible/invalid targets (keep it simple)\n",
    "df = df[df[TARGET].notna()].copy()\n",
    "df = df[df[TARGET] >= 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06275629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Define columns to drop / keep\n",
    "# -----------------------------\n",
    "# ID / non-feature columns to drop\n",
    "DROP_COLS = [\n",
    "    \"county\",           # identifier (high-cardinality); keep only if you explicitly want to model county fixed effects\n",
    "    \"county_code\",      # identifier\n",
    "    \"code\",             # if exists\n",
    "    \"start_time\",       # if exists\n",
    "    \"start_date\",       # if exists\n",
    "    \"date\",             # used for splitting; not as numeric feature in this POC\n",
    "]\n",
    "\n",
    "DROP_COLS = [c for c in DROP_COLS if c in df.columns]\n",
    "\n",
    "# Keep a small ID frame for outputs (what you want to see with predictions)\n",
    "id_cols = [c for c in [\"date\", \"county\", \"county_code\"] if c in df.columns]\n",
    "id_frame = df[id_cols].copy()\n",
    "\n",
    "# Build feature matrix\n",
    "X = df.drop(columns=DROP_COLS + [TARGET], errors=\"ignore\")\n",
    "y = df[TARGET].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4694697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Identify categorical/numeric columns\n",
    "# -----------------------------\n",
    "# 'ar_intensity' is commonly categorical (e.g., AR2/AR3/AR4).\n",
    "# We'll treat object/category dtype columns as categorical automatically.\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9df674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) Preprocessing + Model\n",
    "# -----------------------------\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# We model log1p(y) for stability on long-tailed customer impact\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"ridge\", Ridge(alpha=1.0, random_state=0)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf7d6be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TimeSeriesSplit CV metrics (original scale):\n",
      "   fold          mae     rmsle\n",
      "0     1  3792.099426  1.676775\n",
      "1     2  4356.355289  1.264583\n",
      "2     3  1299.735307  0.958647\n",
      "3     4  2225.209108  1.445127\n",
      "\n",
      "CV averages:\n",
      "              mean\n",
      "mae    2918.349782\n",
      "rmsle     1.336283\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6) Time-based CV evaluation + out-of-fold predictions\n",
    "# -----------------------------\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "oof_pred = np.full(shape=len(df), fill_value=np.nan, dtype=float)\n",
    "\n",
    "fold_metrics = []\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X), start=1):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Fit on log1p(y)\n",
    "    model.fit(X_train, np.log1p(y_train))\n",
    "\n",
    "    # Predict, then invert transform\n",
    "    pred_log = model.predict(X_test)\n",
    "    pred = np.expm1(pred_log)\n",
    "    pred = np.clip(pred, 0, None)\n",
    "\n",
    "    oof_pred[test_idx] = pred\n",
    "\n",
    "    # Metrics on original scale\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "\n",
    "    # RMSLE: safe even if y has zeros; pred is clipped >= 0\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y_test, pred))\n",
    "\n",
    "    fold_metrics.append({\"fold\": fold, \"mae\": mae, \"rmsle\": rmsle})\n",
    "\n",
    "metrics_df = pd.DataFrame(fold_metrics)\n",
    "print(\"\\nTimeSeriesSplit CV metrics (original scale):\")\n",
    "print(metrics_df)\n",
    "print(\"\\nCV averages:\")\n",
    "print(metrics_df[[\"mae\", \"rmsle\"]].mean().to_frame(\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d84ba8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved predictions to: conditional_impact_predictions.csv\n",
      "\n",
      "Preview:\n",
      "        date           county  county_code  y_true_total_customers  \\\n",
      "0 2022-12-31           Amador         6005             2693.000000   \n",
      "1 2022-12-31        El Dorado         6017             1321.333333   \n",
      "2 2022-12-31             Napa         6055             1205.000000   \n",
      "3 2022-12-31       Sacramento         6067              207.250000   \n",
      "4 2022-12-31  San Luis Obispo         6079              512.666667   \n",
      "5 2022-12-31           Solano         6095              365.625000   \n",
      "6 2022-12-31           Sonoma         6097              585.666667   \n",
      "7 2023-01-07      San Joaquin         6077              948.500000   \n",
      "8 2023-01-07           Tehama         6103              392.583333   \n",
      "9 2023-01-07         Monterey         6053             1660.985294   \n",
      "\n",
      "   pred_fit_all_total_customers  \n",
      "0                   1389.358955  \n",
      "1                    936.226238  \n",
      "2                    549.115033  \n",
      "3                    461.345129  \n",
      "4                   4107.565263  \n",
      "5                    827.407783  \n",
      "6                    676.754351  \n",
      "7                    827.887542  \n",
      "8                    456.686124  \n",
      "9                   1271.341312  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# -----------------------------\n",
    "# 7) Fit final model on all data + produce final predictions\n",
    "# -----------------------------\n",
    "model.fit(X, np.log1p(y))\n",
    "final_pred = np.expm1(model.predict(X))\n",
    "final_pred = np.clip(final_pred, 0, None)\n",
    "# -----------------------------\n",
    "# 8) Output predictions\n",
    "# -----------------------------\n",
    "out = id_frame.copy()\n",
    "out[\"y_true_total_customers\"] = y.values\n",
    "out[\"pred_fit_all_total_customers\"] = final_pred  # in-sample fitted predictions (for sanity check only)\n",
    "\n",
    "\n",
    "OUT_PATH = \"conditional_impact_predictions.csv\"\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "print(f\"\\nSaved predictions to: {OUT_PATH}\")\n",
    "print(\"\\nPreview:\")\n",
    "print(out.head(10))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d829d",
   "metadata": {},
   "source": [
    "## 2. XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9c08d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 224, Features: 33\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# XGBoost POC – Data Preparation\n",
    "# ================================\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(\"cleaned_primary_table/conditional_impact_primary_table.csv\")\n",
    "\n",
    "# Date handling (for time-based CV)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Target\n",
    "TARGET = \"total_customers\"\n",
    "y = df[TARGET].astype(float)\n",
    "\n",
    "# Drop ID / non-feature columns (keep consistent with Ridge)\n",
    "DROP_COLS = [\n",
    "    \"county\",\n",
    "    \"county_code\",\n",
    "    \"code\",\n",
    "    \"start_time\",\n",
    "    \"start_date\",\n",
    "    \"date\",\n",
    "]\n",
    "\n",
    "DROP_COLS = [c for c in DROP_COLS if c in df.columns]\n",
    "\n",
    "X = df.drop(columns=DROP_COLS + [TARGET], errors=\"ignore\")\n",
    "\n",
    "# Log-transform target for stability\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "print(f\"Samples: {len(X)}, Features: {X.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d8d0423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost OOF Performance (Original Scale) ===\n",
      "OOF MAE   : 2193.64\n",
      "OOF RMSLE : 1.1581\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# XGBoost Experiment (CV)\n",
    "# ================================\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_log_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Identify feature types\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "# Preprocessing (intentionally simple)\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# Conservative XGB for small-sample regime\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=5,\n",
    "    reg_lambda=1.0,\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=0,\n",
    "    n_jobs=4,\n",
    ")\n",
    "\n",
    "model_xgb = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"xgb\", xgb),\n",
    "])\n",
    "\n",
    "# Time-based CV\n",
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "oof_pred = np.full(len(X), np.nan)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_log = y_log.iloc[train_idx]\n",
    "\n",
    "    model_xgb.fit(X_train, y_train_log)\n",
    "\n",
    "    pred_log = model_xgb.predict(X_test)\n",
    "    pred = np.expm1(pred_log)\n",
    "    pred = np.clip(pred, 0, None)\n",
    "\n",
    "    oof_pred[test_idx] = pred\n",
    "\n",
    "# Metrics (NO R²)\n",
    "mask = ~np.isnan(oof_pred)\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, np.clip(y_pred, 0, None)))\n",
    "\n",
    "print(\"=== XGBoost OOF Performance (Original Scale) ===\")\n",
    "print(f\"OOF MAE   : {mean_absolute_error(y[mask], oof_pred[mask]):.2f}\")\n",
    "print(f\"OOF RMSLE : {rmsle(y[mask], oof_pred[mask]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d213e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final XGB predictions to: conditional_impact_xgb_predictions.csv\n",
      "        date           county  actual_total_customers  \\\n",
      "0 2022-12-31           Amador             2693.000000   \n",
      "1 2022-12-31        El Dorado             1321.333333   \n",
      "2 2022-12-31             Napa             1205.000000   \n",
      "3 2022-12-31       Sacramento              207.250000   \n",
      "4 2022-12-31  San Luis Obispo              512.666667   \n",
      "5 2022-12-31           Solano              365.625000   \n",
      "6 2022-12-31           Sonoma              585.666667   \n",
      "7 2023-01-07      San Joaquin              948.500000   \n",
      "8 2023-01-07           Tehama              392.583333   \n",
      "9 2023-01-07         Monterey             1660.985294   \n",
      "\n",
      "   pred_total_customers_xgb  \n",
      "0               1811.578613  \n",
      "1               1393.087646  \n",
      "2               1106.792114  \n",
      "3                275.534180  \n",
      "4                624.131531  \n",
      "5                430.788513  \n",
      "6                556.895447  \n",
      "7                985.623047  \n",
      "8                472.271362  \n",
      "9               1520.844238  \n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Final XGBoost Fit + Prediction\n",
    "# ================================\n",
    "\n",
    "# Fit on full dataset\n",
    "model_xgb.fit(X, y_log)\n",
    "\n",
    "# Final predictions (in-sample, for downstream use)\n",
    "final_pred_log = model_xgb.predict(X)\n",
    "final_pred = np.expm1(final_pred_log)\n",
    "final_pred = np.clip(final_pred, 0, None)\n",
    "\n",
    "# Output predictions with ground truth\n",
    "out = df[[\"date\", \"county\"]].copy()\n",
    "out[\"actual_total_customers\"] = df[\"total_customers\"].values\n",
    "out[\"pred_total_customers_xgb\"] = final_pred\n",
    "\n",
    "OUT_PATH = \"conditional_impact_xgb_predictions.csv\"\n",
    "out.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved final XGB predictions to: {OUT_PATH}\")\n",
    "print(out.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e741f72",
   "metadata": {},
   "source": [
    "## 3. Merge with proability forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57504eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = pd.read_csv(\"risk_predictions_downloaded/risk_model_score_predictions.csv\")\n",
    "risk['date'] = pd.to_datetime(risk['date'])\n",
    "impact = out.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dac1e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = pd.merge(risk, impact, left_on = [\"date\", \"county_name\"], right_on = [\"date\", \"county\"], how = \"left\")\n",
    "combined_results.drop(columns=[\"county\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28347cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_name</th>\n",
       "      <th>date</th>\n",
       "      <th>risk score</th>\n",
       "      <th>predicted_risk_score</th>\n",
       "      <th>actual_total_customers</th>\n",
       "      <th>pred_total_customers_xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Amador</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.966251</td>\n",
       "      <td>2693.000000</td>\n",
       "      <td>1811.578613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>El Dorado</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>1321.333333</td>\n",
       "      <td>1393.087646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>Napa</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783266</td>\n",
       "      <td>1205.000000</td>\n",
       "      <td>1106.792114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>Sacramento</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.600284</td>\n",
       "      <td>207.250000</td>\n",
       "      <td>275.534180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>San Luis Obispo</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.653017</td>\n",
       "      <td>512.666667</td>\n",
       "      <td>624.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>Napa</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.315728</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>321.671021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6212</th>\n",
       "      <td>Riverside</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.503726</td>\n",
       "      <td>1091.647059</td>\n",
       "      <td>1309.596191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6429</th>\n",
       "      <td>San Luis Obispo</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>515.111111</td>\n",
       "      <td>519.174194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6491</th>\n",
       "      <td>Santa Barbara</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.435082</td>\n",
       "      <td>338.000000</td>\n",
       "      <td>368.260498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6677</th>\n",
       "      <td>Solano</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447342</td>\n",
       "      <td>499.705882</td>\n",
       "      <td>566.200317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          county_name       date  risk score  predicted_risk_score  \\\n",
       "92             Amador 2022-12-31         1.0              0.966251   \n",
       "278         El Dorado 2022-12-31         1.0              0.991054   \n",
       "867              Napa 2022-12-31         1.0              0.783266   \n",
       "1053       Sacramento 2022-12-31         1.0              0.600284   \n",
       "1239  San Luis Obispo 2022-12-31         1.0              0.653017   \n",
       "...               ...        ...         ...                   ...   \n",
       "6057             Napa 2023-03-01         1.0              0.315728   \n",
       "6212        Riverside 2023-03-01         1.0              0.503726   \n",
       "6429  San Luis Obispo 2023-03-01         1.0              0.403509   \n",
       "6491    Santa Barbara 2023-03-01         1.0              0.435082   \n",
       "6677           Solano 2023-03-01         1.0              0.447342   \n",
       "\n",
       "      actual_total_customers  pred_total_customers_xgb  \n",
       "92               2693.000000               1811.578613  \n",
       "278              1321.333333               1393.087646  \n",
       "867              1205.000000               1106.792114  \n",
       "1053              207.250000                275.534180  \n",
       "1239              512.666667                624.131531  \n",
       "...                      ...                       ...  \n",
       "6057              218.000000                321.671021  \n",
       "6212             1091.647059               1309.596191  \n",
       "6429              515.111111                519.174194  \n",
       "6491              338.000000                368.260498  \n",
       "6677              499.705882                566.200317  \n",
       "\n",
       "[224 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results[combined_results[\"risk score\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f40b9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results.to_csv(\"combined_risk_and_impact_predictions.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
